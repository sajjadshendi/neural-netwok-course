{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 161,
      "metadata": {
        "id": "9fgFqT92vFHv"
      },
      "outputs": [],
      "source": [
        "from google.colab import files\n",
        "import os\n",
        "import pandas as pd\n",
        "from sklearn.model_selection import train_test_split\n",
        "from torch.nn.utils.rnn import pad_sequence\n",
        "import torch\n",
        "import torchvision\n",
        "from torch import optim\n",
        "from torch import nn\n",
        "from torch.utils.data import Dataset\n",
        "import random\n",
        "import math"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "NJxBndNVyoRu"
      },
      "source": [
        "import necessary libraries"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 162,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 292
        },
        "id": "KLVBI_Kryvkp",
        "outputId": "a5b15f13-015a-418e-9eb4-1d11dd802bac"
      },
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "\n",
              "     <input type=\"file\" id=\"files-59f8d69e-f2c2-476e-bfde-8336e86890a1\" name=\"files[]\" multiple disabled\n",
              "        style=\"border:none\" />\n",
              "     <output id=\"result-59f8d69e-f2c2-476e-bfde-8336e86890a1\">\n",
              "      Upload widget is only available when the cell has been executed in the\n",
              "      current browser session. Please rerun this cell to enable.\n",
              "      </output>\n",
              "      <script>// Copyright 2017 Google LLC\n",
              "//\n",
              "// Licensed under the Apache License, Version 2.0 (the \"License\");\n",
              "// you may not use this file except in compliance with the License.\n",
              "// You may obtain a copy of the License at\n",
              "//\n",
              "//      http://www.apache.org/licenses/LICENSE-2.0\n",
              "//\n",
              "// Unless required by applicable law or agreed to in writing, software\n",
              "// distributed under the License is distributed on an \"AS IS\" BASIS,\n",
              "// WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n",
              "// See the License for the specific language governing permissions and\n",
              "// limitations under the License.\n",
              "\n",
              "/**\n",
              " * @fileoverview Helpers for google.colab Python module.\n",
              " */\n",
              "(function(scope) {\n",
              "function span(text, styleAttributes = {}) {\n",
              "  const element = document.createElement('span');\n",
              "  element.textContent = text;\n",
              "  for (const key of Object.keys(styleAttributes)) {\n",
              "    element.style[key] = styleAttributes[key];\n",
              "  }\n",
              "  return element;\n",
              "}\n",
              "\n",
              "// Max number of bytes which will be uploaded at a time.\n",
              "const MAX_PAYLOAD_SIZE = 100 * 1024;\n",
              "\n",
              "function _uploadFiles(inputId, outputId) {\n",
              "  const steps = uploadFilesStep(inputId, outputId);\n",
              "  const outputElement = document.getElementById(outputId);\n",
              "  // Cache steps on the outputElement to make it available for the next call\n",
              "  // to uploadFilesContinue from Python.\n",
              "  outputElement.steps = steps;\n",
              "\n",
              "  return _uploadFilesContinue(outputId);\n",
              "}\n",
              "\n",
              "// This is roughly an async generator (not supported in the browser yet),\n",
              "// where there are multiple asynchronous steps and the Python side is going\n",
              "// to poll for completion of each step.\n",
              "// This uses a Promise to block the python side on completion of each step,\n",
              "// then passes the result of the previous step as the input to the next step.\n",
              "function _uploadFilesContinue(outputId) {\n",
              "  const outputElement = document.getElementById(outputId);\n",
              "  const steps = outputElement.steps;\n",
              "\n",
              "  const next = steps.next(outputElement.lastPromiseValue);\n",
              "  return Promise.resolve(next.value.promise).then((value) => {\n",
              "    // Cache the last promise value to make it available to the next\n",
              "    // step of the generator.\n",
              "    outputElement.lastPromiseValue = value;\n",
              "    return next.value.response;\n",
              "  });\n",
              "}\n",
              "\n",
              "/**\n",
              " * Generator function which is called between each async step of the upload\n",
              " * process.\n",
              " * @param {string} inputId Element ID of the input file picker element.\n",
              " * @param {string} outputId Element ID of the output display.\n",
              " * @return {!Iterable<!Object>} Iterable of next steps.\n",
              " */\n",
              "function* uploadFilesStep(inputId, outputId) {\n",
              "  const inputElement = document.getElementById(inputId);\n",
              "  inputElement.disabled = false;\n",
              "\n",
              "  const outputElement = document.getElementById(outputId);\n",
              "  outputElement.innerHTML = '';\n",
              "\n",
              "  const pickedPromise = new Promise((resolve) => {\n",
              "    inputElement.addEventListener('change', (e) => {\n",
              "      resolve(e.target.files);\n",
              "    });\n",
              "  });\n",
              "\n",
              "  const cancel = document.createElement('button');\n",
              "  inputElement.parentElement.appendChild(cancel);\n",
              "  cancel.textContent = 'Cancel upload';\n",
              "  const cancelPromise = new Promise((resolve) => {\n",
              "    cancel.onclick = () => {\n",
              "      resolve(null);\n",
              "    };\n",
              "  });\n",
              "\n",
              "  // Wait for the user to pick the files.\n",
              "  const files = yield {\n",
              "    promise: Promise.race([pickedPromise, cancelPromise]),\n",
              "    response: {\n",
              "      action: 'starting',\n",
              "    }\n",
              "  };\n",
              "\n",
              "  cancel.remove();\n",
              "\n",
              "  // Disable the input element since further picks are not allowed.\n",
              "  inputElement.disabled = true;\n",
              "\n",
              "  if (!files) {\n",
              "    return {\n",
              "      response: {\n",
              "        action: 'complete',\n",
              "      }\n",
              "    };\n",
              "  }\n",
              "\n",
              "  for (const file of files) {\n",
              "    const li = document.createElement('li');\n",
              "    li.append(span(file.name, {fontWeight: 'bold'}));\n",
              "    li.append(span(\n",
              "        `(${file.type || 'n/a'}) - ${file.size} bytes, ` +\n",
              "        `last modified: ${\n",
              "            file.lastModifiedDate ? file.lastModifiedDate.toLocaleDateString() :\n",
              "                                    'n/a'} - `));\n",
              "    const percent = span('0% done');\n",
              "    li.appendChild(percent);\n",
              "\n",
              "    outputElement.appendChild(li);\n",
              "\n",
              "    const fileDataPromise = new Promise((resolve) => {\n",
              "      const reader = new FileReader();\n",
              "      reader.onload = (e) => {\n",
              "        resolve(e.target.result);\n",
              "      };\n",
              "      reader.readAsArrayBuffer(file);\n",
              "    });\n",
              "    // Wait for the data to be ready.\n",
              "    let fileData = yield {\n",
              "      promise: fileDataPromise,\n",
              "      response: {\n",
              "        action: 'continue',\n",
              "      }\n",
              "    };\n",
              "\n",
              "    // Use a chunked sending to avoid message size limits. See b/62115660.\n",
              "    let position = 0;\n",
              "    do {\n",
              "      const length = Math.min(fileData.byteLength - position, MAX_PAYLOAD_SIZE);\n",
              "      const chunk = new Uint8Array(fileData, position, length);\n",
              "      position += length;\n",
              "\n",
              "      const base64 = btoa(String.fromCharCode.apply(null, chunk));\n",
              "      yield {\n",
              "        response: {\n",
              "          action: 'append',\n",
              "          file: file.name,\n",
              "          data: base64,\n",
              "        },\n",
              "      };\n",
              "\n",
              "      let percentDone = fileData.byteLength === 0 ?\n",
              "          100 :\n",
              "          Math.round((position / fileData.byteLength) * 100);\n",
              "      percent.textContent = `${percentDone}% done`;\n",
              "\n",
              "    } while (position < fileData.byteLength);\n",
              "  }\n",
              "\n",
              "  // All done.\n",
              "  yield {\n",
              "    response: {\n",
              "      action: 'complete',\n",
              "    }\n",
              "  };\n",
              "}\n",
              "\n",
              "scope.google = scope.google || {};\n",
              "scope.google.colab = scope.google.colab || {};\n",
              "scope.google.colab._files = {\n",
              "  _uploadFiles,\n",
              "  _uploadFilesContinue,\n",
              "};\n",
              "})(self);\n",
              "</script> "
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Saving kaggle.json to kaggle (2).json\n",
            "persian-wikipedia-dataset.zip: Skipping, found more recently modified local copy (use --force to force download)\n",
            "Archive:  /content/persian-wikipedia-dataset.zip\n",
            "replace Persian-WikiText-1.txt? [y]es, [n]o, [A]ll, [N]one, [r]ename: A\n",
            "  inflating: Persian-WikiText-1.txt  \n",
            "  inflating: Persian-WikiText-2.txt  \n",
            "  inflating: Persian-WikiText-3.txt  \n",
            "  inflating: Persian-WikiText-4.txt  \n",
            "  inflating: Persian-WikiText-5.txt  \n",
            "  inflating: Persian-WikiText-6.txt  \n",
            "  inflating: Persian-WikiText-7.txt  \n",
            "  inflating: Persian-WikiText-8.txt  \n",
            "  inflating: Persian-WikiText-9.txt  \n"
          ]
        }
      ],
      "source": [
        "files.upload()\n",
        "os.system(\"pip install kaggle\")\n",
        "os.system(\"mkdir -p ~/.kaggle\")\n",
        "os.system(\"cp kaggle.json ~/.kaggle/\")\n",
        "os.system(\"chmod 600 ~/.kaggle/kaggle.json\")\n",
        "!kaggle datasets download -d miladfa7/persian-wikipedia-dataset\n",
        "!unzip /content/persian-wikipedia-dataset.zip"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "N_0G6uKv0pBV"
      },
      "source": [
        "import data files from kaggle"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 163,
      "metadata": {
        "id": "sCUA8g230l4r"
      },
      "outputs": [],
      "source": [
        "sentences1 = open(\"/content/Persian-WikiText-1.txt\", encoding = 'utf8').read().replace(\"\\n\",\"\").split('.')\n",
        "sentences2 = open(\"/content/Persian-WikiText-2.txt\", encoding = 'utf8').read().replace(\"\\n\",\"\").split('.')\n",
        "sentences3 = open(\"/content/Persian-WikiText-3.txt\", encoding = 'utf8').read().replace(\"\\n\",\"\").split('.')\n",
        "sentences4 = open(\"/content/Persian-WikiText-4.txt\", encoding = 'utf8').read().replace(\"\\n\",\"\").split('.')\n",
        "sentences5 = open(\"/content/Persian-WikiText-5.txt\", encoding = 'utf8').read().replace(\"\\n\",\"\").split('.')\n",
        "sentences6 = open(\"/content/Persian-WikiText-6.txt\", encoding = 'utf8').read().replace(\"\\n\",\"\").split('.')\n",
        "sentences7 = open(\"/content/Persian-WikiText-7.txt\", encoding = 'utf8').read().replace(\"\\n\",\"\").split('.')\n",
        "sentences8 = open(\"/content/Persian-WikiText-8.txt\", encoding = 'utf8').read().replace(\"\\n\",\"\").split('.')\n",
        "sentences9 = open(\"/content/Persian-WikiText-9.txt\", encoding = 'utf8').read().replace(\"\\n\",\"\").split('.')\n",
        "sentences = sentences1 +sentences2 + sentences3 + sentences4 + sentences5 + sentences6 + sentences7 + sentences8 + sentences9"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "تمام نه فایل موجود را باز کرده، جملات آن را جدا می کنیم و در انتها تمام جملات را داخل یک آرایه میگذاریم. ولیکن قبل از آن تمام جملاتی که در چند خط هستند را به یک خط می آوریم و اینکه جمله ها را از طریق نقاط پایان جمله شناسایی میکینیم"
      ],
      "metadata": {
        "id": "7qKT-2agOMph"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 164,
      "metadata": {
        "id": "m5j13FCBFFOn"
      },
      "outputs": [],
      "source": [
        "#######################################################\n",
        "#               Define Vocabulary Class\n",
        "#######################################################\n",
        "\n",
        "class Vocabulary:\n",
        "  \n",
        "    \n",
        "    def __init__(self, freq_threshold, max_size):\n",
        "        '''\n",
        "        freq_threshold : the minimum times a word must occur in corpus to be treated in vocab\n",
        "        max_size : max source vocab size. Eg. if set to 10,000, we pick the top 10,000 most frequent words and discard others\n",
        "        '''\n",
        "        #initiate the index to token dict\n",
        "        ## <PAD> -> padding, used for padding the shorter sentences in a batch to match the length of longest sentence in the batch\n",
        "        ## <SOS> -> start token, added in front of each sentence to signify the start of sentence\n",
        "        ## <EOS> -> End of sentence token, added to the end of each sentence to signify the end of sentence\n",
        "        ## <UNK> -> words which are not found in the vocab are replace by this token\n",
        "        self.itos = {0: '<PAD>', 1:'<SOS>', 2:'<EOS>', 3: '<UNK>'}\n",
        "        #initiate the token to index dict\n",
        "        self.stoi = {k:j for j,k in self.itos.items()} \n",
        "        \n",
        "        self.freq_threshold = freq_threshold\n",
        "        self.max_size = max_size\n",
        "    \n",
        "    '''\n",
        "    __len__ is used by dataloader later to create batches\n",
        "    '''\n",
        "    def __len__(self):\n",
        "        return len(self.itos)\n",
        "    \n",
        "    '''\n",
        "    a simple tokenizer to split on space and converts the sentence to list of words\n",
        "    '''\n",
        "    @staticmethod\n",
        "    def tokenizer(text):\n",
        "        return [tok.lower().strip() for tok in text.split(' ')]\n",
        "    \n",
        "    '''\n",
        "    build the vocab: create a dictionary mapping of index to string (itos) and string to index (stoi)\n",
        "    output ex. for stoi -> {'the':5, 'a':6, 'an':7}\n",
        "    '''\n",
        "    def build_vocabulary(self, sentence_list):\n",
        "        #calculate the frequencies of each word first to remove the words with freq < freq_threshold\n",
        "        frequencies = {}  #init the freq dict\n",
        "        idx = 4 #index from which we want our dict to start. We already used 4 indexes for pad, start, end, unk\n",
        "        \n",
        "        #calculate freq of words\n",
        "        for sentence in sentence_list:\n",
        "            for word in self.tokenizer(sentence):\n",
        "                if word not in frequencies.keys():\n",
        "                    frequencies[word]=1\n",
        "                else:\n",
        "                    frequencies[word]+=1\n",
        "                    \n",
        "                    \n",
        "        #limit vocab by removing low freq words\n",
        "        frequencies = {k:v for k,v in frequencies.items() if v>self.freq_threshold} \n",
        "        \n",
        "        #limit vocab to the max_size specified\n",
        "        frequencies = dict(sorted(frequencies.items(), key = lambda x: -x[1])[:self.max_size-idx]) # idx =4 for pad, start, end , unk\n",
        "            \n",
        "        #create vocab\n",
        "        for word in frequencies.keys():\n",
        "            self.stoi[word] = idx\n",
        "            self.itos[idx] = word\n",
        "            idx+=1\n",
        "            \n",
        "    '''\n",
        "    convert the list of words to a list of corresponding indexes\n",
        "    '''    \n",
        "    def numericalize(self, text):\n",
        "        #tokenize text\n",
        "        tokenized_text = self.tokenizer(text)\n",
        "        numericalized_text = []\n",
        "        for token in tokenized_text:\n",
        "            if token in self.stoi.keys():\n",
        "                numericalized_text.append(self.stoi[token])\n",
        "            else: #out-of-vocab words are represented by UNK token index\n",
        "                numericalized_text.append(self.stoi['<UNK>'])\n",
        "                \n",
        "        return numericalized_text"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "تعریف کلاس دیکشنری"
      ],
      "metadata": {
        "id": "0GKhTTbhPETM"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 165,
      "metadata": {
        "id": "X6Ms71u-ab3R"
      },
      "outputs": [],
      "source": [
        "first_of_sentences = []\n",
        "rest_of_the_sentences = []\n",
        "count = 0\n",
        "for sentence in sentences:\n",
        "  if count == 1000:\n",
        "    break\n",
        "  count += 1\n",
        "  splitted_sentence = sentence.split()\n",
        "  first_of_sentences.append(splitted_sentence[0:5])\n",
        "  rest_of_the_sentences.append(splitted_sentence[5:])\n",
        "firsts = []\n",
        "for item in first_of_sentences:\n",
        "  firsts.append(\" \".join(item))\n",
        "rests = []\n",
        "for item in rest_of_the_sentences:\n",
        "  rests.append(\" \".join(item))"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "در ادامه برای ساخت دیتا ست، ما دو تا آرایه میسازیم. آرایه اول بدین صورت ساخته میشود که 5 کلمه ی ابتدای هر جمله را به عنوان یک عضو آرایه میگذاریم و آرایه دوم بدین صورت ساخته میشود که بقیه هر جمله ( به غیر از پنج کلمه اول ) را در آن میگذاریم. گفتنی است که مثلا عضو اول از آرایه اول و عضو اول از آرایه دوم مکمل یکدیگر هستند"
      ],
      "metadata": {
        "id": "q-yS8Vx5PI2m"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 166,
      "metadata": {
        "id": "ytFvKm8DFSBI"
      },
      "outputs": [],
      "source": [
        "voc = Vocabulary(2, 10000)\n",
        "voc.build_vocabulary(sentences)"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "ساخت دیکشنری از روی تمام جملات"
      ],
      "metadata": {
        "id": "rpu4TTmLQA9s"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 167,
      "metadata": {
        "id": "e6vBL3NOfSI4"
      },
      "outputs": [],
      "source": [
        "raw_data = {\n",
        "    'firsts': [first for first in firsts],\n",
        "    'rests': [rest for rest in rests]\n",
        "}\n",
        "\n",
        "df = pd.DataFrame(raw_data, columns = ['firsts', 'rests'])\n",
        "\n",
        "train = pd.DataFrame({\n",
        "    'firsts': df['firsts'][0:800],\n",
        "    'rests': df['rests'][0:800]\n",
        "})\n",
        "\n",
        "test = pd.DataFrame({\n",
        "    'firsts': df['firsts'][800:1000],\n",
        "    'rests': df['rests'][800:1000]\n",
        "})\n"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "یک دیتا فریم پانداز میسازیم که آرایه اول در قسمت های قبلی را در کنار آرایه دوم قرار دهد یعنی داده ورودی اعضای آرایه اول باشند و اعضای آرایه دوم لیبل های مربوط به ورودی ها باشند. در ضمن در این مرحله داده آموزش و تست را نیز جدا میکنیم"
      ],
      "metadata": {
        "id": "FHoObBieQQrb"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 168,
      "metadata": {
        "id": "MtODeRGRIcR3"
      },
      "outputs": [],
      "source": [
        "\n",
        "#######################################################\n",
        "#               Define Train_Dataset class\n",
        "#######################################################\n",
        "\n",
        "class Train_Dataset(Dataset):\n",
        "    '''\n",
        "    Initiating Variables\n",
        "    df: the training dataframe\n",
        "    source_column : the name of source text column in the dataframe\n",
        "    target_columns : the name of target text column in the dataframe\n",
        "    transform : If we want to add any augmentation\n",
        "    '''\n",
        "    \n",
        "    def __init__(self, df, source_column, target_column, voc, transform=None):\n",
        "    \n",
        "        self.df = df\n",
        "        self.transform = transform\n",
        "        \n",
        "        #get source and target texts\n",
        "        self.source_texts = self.df[source_column]\n",
        "        self.target_texts = self.df[target_column]\n",
        "        \n",
        "        \n",
        "        self.vocab = voc\n",
        "        \n",
        "    def __len__(self):\n",
        "        return len(self.df)\n",
        "    \n",
        "    '''\n",
        "    __getitem__ runs on 1 example at a time. Here, we get an example at index and return its numericalize source and\n",
        "    target values using the vocabulary objects we created in __init__\n",
        "    '''\n",
        "    def __getitem__(self, index):\n",
        "        source_text = self.source_texts[index]\n",
        "        target_text = self.target_texts[index]\n",
        "        \n",
        "        if self.transform is not None:\n",
        "            source_text = self.transform(source_text)\n",
        "            \n",
        "        #numericalize texts ['<SOS>','cat', 'in', 'a', 'bag','<EOS>'] -> [1,12,2,9,24,2]\n",
        "        numerialized_source = [self.vocab.stoi[\"<SOS>\"]]\n",
        "        numerialized_source += self.vocab.numericalize(source_text)\n",
        "        numerialized_source.append(self.vocab.stoi[\"<EOS>\"])\n",
        "    \n",
        "        numerialized_target = [self.vocab.stoi[\"<SOS>\"]]\n",
        "        numerialized_target += self.vocab.numericalize(target_text)\n",
        "        numerialized_target.append(self.vocab.stoi[\"<EOS>\"])\n",
        "        \n",
        "        #convert the list to tensor and return\n",
        "        return torch.tensor(numerialized_source), torch.tensor(numerialized_target)"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "در اینجا دیتا ست را تعریف میکنیم به گونه ای که  جملات داخل دیتا فریم پانداز به کلمه ها تقسیم شوند و هر کلمه متناظر با یک عدد در دیکشنری شود"
      ],
      "metadata": {
        "id": "_-6Bx_DtQ2wQ"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 169,
      "metadata": {
        "id": "H_rfqrLYY90d",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "99814e1b-a07e-4f6e-d16e-6909192be063"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "(tensor([   1,   15,   17, 4751,    3,    3,    2]), tensor([   1,    3,   17, 1610,    3, 5830,    3, 1422,    6,   71,    3,    5,\n",
            "           3,   18, 4127, 9830,    3, 1794,   31, 1252,   16, 2856,  456,    5,\n",
            "         521,  453,    9,   10,   16,  521,  318, 6380,  273,   11,   29,    5,\n",
            "          67,  806,   10,    6, 1748,    5, 1252, 1112,   77,   45,   11,  172,\n",
            "        2557,   19,   13, 6945,    5, 3385,   41,    2]))\n",
            "(tensor([   1,   38, 1610, 5830,  700,   30,    2]), tensor([   1, 1770,    9,   10,    8,  700,   14, 1610,    3,    3,    5,    3,\n",
            "           3,    3,   35,    3,  126,   20,    9,    2]))\n",
            "(tensor([   1,  470, 1610, 5830, 5476,    5,    2]), tensor([   1,  630,  215,   18, 4127,   16, 2856,  456,    6,  789,   88,   14,\n",
            "           3,  388,    9,    2]))\n",
            "(tensor([   1, 1610, 2544,  483,    4,   81,    2]), tensor([   1,  560,  655, 1704,    3,  283,    3,    6,   71,    3,   24,    3,\n",
            "        1964,    3,  273,   23,    2]))\n",
            "(tensor([   1, 1209, 3393,   19,    3,    3,    2]), tensor([ 1,  5,  3,  3, 91,  2]))\n"
          ]
        }
      ],
      "source": [
        "train_dataset = Train_Dataset(train, 'firsts', 'rests', voc)\n",
        "for i in range(5):\n",
        "  print(train_dataset[i])"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "دیتاست را میسازیم"
      ],
      "metadata": {
        "id": "WidsBAe_RNPR"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 170,
      "metadata": {
        "id": "nnjftEFZdPVA"
      },
      "outputs": [],
      "source": [
        "\n",
        "#######################################################\n",
        "#               Define Dataset Class\n",
        "#######################################################\n",
        "\n",
        "class Validation_Dataset:\n",
        "    def __init__(self, train_dataset, df, source_column, target_column, transform = None):\n",
        "        self.df = df\n",
        "        self.transform = transform\n",
        "        \n",
        "        #train dataset will be used as lookup for vocab\n",
        "        self.train_dataset = train_dataset\n",
        "        \n",
        "        #get source and target texts\n",
        "        self.source_texts = self.df[source_column]\n",
        "        self.target_texts = self.df[target_column]\n",
        "    \n",
        "    def __len__(self):\n",
        "        return len(self.df)\n",
        "    \n",
        "    def __getitem__(self,index):\n",
        "        source_text = self.source_texts[index]\n",
        "        #print(source_text)\n",
        "        target_text = self.target_texts[index]\n",
        "        #print(target_text)\n",
        "        if self.transform is not None:\n",
        "            source_text = self.transform(source_text)\n",
        "            \n",
        "        #numericalize texts ['<SOS>','cat', 'in', 'a', 'bag','<EOS>'] -> [1,12,2,9,24,2]\n",
        "        numerialized_source = [self.train_dataset.vocab.stoi[\"<SOS>\"]]\n",
        "        numerialized_source += self.train_dataset.vocab.numericalize(source_text)\n",
        "        numerialized_source.append(self.train_dataset.vocab.stoi[\"<EOS>\"])\n",
        "    \n",
        "        numerialized_target = [self.train_dataset.vocab.stoi[\"<SOS>\"]]\n",
        "        numerialized_target += self.train_dataset.vocab.numericalize(target_text)\n",
        "        numerialized_target.append(self.train_dataset.vocab.stoi[\"<EOS>\"])\n",
        "        #print(numerialized_source)\n",
        "        return torch.tensor(numerialized_source), torch.tensor(numerialized_target)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "دیتاست مربوط به داده های تست را تعریف میکینم"
      ],
      "metadata": {
        "id": "RnjUEfNCRvTP"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "test_dataset = Validation_Dataset(train_dataset, df, 'firsts', 'rests')\n",
        "for i in range(5):\n",
        "  print(train_dataset[i])"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "6Z5LQJemRy9_",
        "outputId": "ee3d3ce1-81dc-408a-d515-b9df51f0fe0f"
      },
      "execution_count": 171,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "(tensor([   1,   15,   17, 4751,    3,    3,    2]), tensor([   1,    3,   17, 1610,    3, 5830,    3, 1422,    6,   71,    3,    5,\n",
            "           3,   18, 4127, 9830,    3, 1794,   31, 1252,   16, 2856,  456,    5,\n",
            "         521,  453,    9,   10,   16,  521,  318, 6380,  273,   11,   29,    5,\n",
            "          67,  806,   10,    6, 1748,    5, 1252, 1112,   77,   45,   11,  172,\n",
            "        2557,   19,   13, 6945,    5, 3385,   41,    2]))\n",
            "(tensor([   1,   38, 1610, 5830,  700,   30,    2]), tensor([   1, 1770,    9,   10,    8,  700,   14, 1610,    3,    3,    5,    3,\n",
            "           3,    3,   35,    3,  126,   20,    9,    2]))\n",
            "(tensor([   1,  470, 1610, 5830, 5476,    5,    2]), tensor([   1,  630,  215,   18, 4127,   16, 2856,  456,    6,  789,   88,   14,\n",
            "           3,  388,    9,    2]))\n",
            "(tensor([   1, 1610, 2544,  483,    4,   81,    2]), tensor([   1,  560,  655, 1704,    3,  283,    3,    6,   71,    3,   24,    3,\n",
            "        1964,    3,  273,   23,    2]))\n",
            "(tensor([   1, 1209, 3393,   19,    3,    3,    2]), tensor([ 1,  5,  3,  3, 91,  2]))\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "دیتاست داده های تست را میسازیم"
      ],
      "metadata": {
        "id": "woENdC_LSMh6"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 172,
      "metadata": {
        "id": "IiUlivJuexM8"
      },
      "outputs": [],
      "source": [
        "\n",
        "#######################################################\n",
        "#            Define Dataloader Functions\n",
        "#######################################################\n",
        "\n",
        "\n",
        "def get_train_loader(dataset, batch_size):\n",
        "    #define loader\n",
        "    loader = torch.utils.data.DataLoader(dataset, batch_size = batch_size) \n",
        "    return loader\n",
        "\n",
        "def get_valid_loader(dataset, batch_size):\n",
        "    #define loader\n",
        "    loader = torch.utils.data.DataLoader(dataset, batch_size = batch_size)\n",
        "    return loader"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 173,
      "metadata": {
        "id": "rzyJcvJ0fCgo"
      },
      "outputs": [],
      "source": [
        "train_loader = get_train_loader(train_dataset, 1)\n",
        "test_loader = get_valid_loader(test_dataset, 1)"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "دیتا لودر ها را میسازیم"
      ],
      "metadata": {
        "id": "AVKPzyZoSs8t"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 174,
      "metadata": {
        "id": "cFFasXt0H2bS"
      },
      "outputs": [],
      "source": [
        "class Encoder(nn.Module):\n",
        "    def __init__(self, input_dim, emb_dim, hid_dim, dropout):\n",
        "        super().__init__()\n",
        "\n",
        "        self.hid_dim = hid_dim\n",
        "        \n",
        "        self.embedding = nn.Embedding(input_dim, emb_dim) #no dropout as only one layer!\n",
        "        \n",
        "        self.rnn = nn.GRU(emb_dim, hid_dim)\n",
        "        \n",
        "        self.dropout = nn.Dropout(dropout)\n",
        "        \n",
        "    def forward(self, src, hid_data):\n",
        "        \n",
        "        #src.shape = [batch size, src len]\n",
        "        \n",
        "        embedded = self.dropout(self.embedding(src))\n",
        "        \n",
        "        outputs, hidden = self.rnn(embedded, hid_data) #no cell state!\n",
        "        \n",
        "        \n",
        "        #outputs are always from the top hidden layer\n",
        "        \n",
        "        return hidden"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "تعریف انکودر"
      ],
      "metadata": {
        "id": "jSDG3b4bTBDC"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 175,
      "metadata": {
        "id": "LmmS17afJD6C"
      },
      "outputs": [],
      "source": [
        "class Decoder(nn.Module):\n",
        "    def __init__(self, output_dim, emb_dim, hid_dim, dropout):\n",
        "        super().__init__()\n",
        "\n",
        "        self.hid_dim = hid_dim\n",
        "        self.output_dim = output_dim\n",
        "        \n",
        "        self.embedding = nn.Embedding(output_dim, emb_dim)\n",
        "        \n",
        "        self.rnn = nn.GRU(emb_dim + hid_dim, hid_dim)\n",
        "        \n",
        "        self.fc_out = nn.Linear(emb_dim + hid_dim * 2, output_dim)\n",
        "        \n",
        "        self.dropout = nn.Dropout(dropout)\n",
        "        \n",
        "    def forward(self, input, hidden, context):\n",
        "        \n",
        "        #input = ورودی هر دفعه همان خروجی قبلی است که متناظر با یک کلمه است\n",
        "        #hidden = انتقالی از زمان قبلی\n",
        "        #context = latent_space\n",
        "        \n",
        "        #n layers and n directions in the decoder will both always be 1, therefore:\n",
        "        \n",
        "        input = input.unsqueeze(0)\n",
        "        \n",
        "        \n",
        "        embedded = self.dropout(self.embedding(input))\n",
        "        \n",
        "                \n",
        "        emb_con = torch.cat((embedded, context), dim = 1)\n",
        "            \n",
        "            \n",
        "        output, hidden = self.rnn(emb_con, hidden)\n",
        "        \n",
        "        \n",
        "        #seq len, n layers and n directions will always be 1 in the decoder, therefore:\n",
        "        \n",
        "        output = torch.cat((embedded, hidden, context), \n",
        "                           dim = 1)\n",
        "        \n",
        "        \n",
        "        prediction = self.fc_out(output)\n",
        "        \n",
        "        \n",
        "        return prediction, hidden"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "تعریف دیکودر"
      ],
      "metadata": {
        "id": "8UKpZVi-T1Vu"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 176,
      "metadata": {
        "id": "u7qhDixuJgci"
      },
      "outputs": [],
      "source": [
        "class Seq2Seq(nn.Module):\n",
        "    def __init__(self, encoder, decoder, device):\n",
        "        super().__init__()\n",
        "        \n",
        "        self.encoder = encoder\n",
        "        self.decoder = decoder\n",
        "        self.device = device\n",
        "        \n",
        "        assert encoder.hid_dim == decoder.hid_dim, \\\n",
        "            \"Hidden dimensions of encoder and decoder must be equal!\"\n",
        "        \n",
        "    def forward(self, src, trg, teacher_forcing_ratio = 0.5):\n",
        "        \n",
        "        #src = [bach size, src len]\n",
        "        #trg = [batch size, trg len]\n",
        "        #teacher_forcing_ratio is probability to use teacher forcing\n",
        "        #e.g. if teacher_forcing_ratio is 0.75 we use ground-truth inputs 75% of the time\n",
        "        \n",
        "        input_length = src.shape[1]\n",
        "        batch_size = trg.shape[0]\n",
        "        trg_len = trg.shape[1]\n",
        "        trg_vocab_size = self.decoder.output_dim\n",
        "        \n",
        "        #tensor to store decoder outputs\n",
        "        outputs = torch.zeros(trg_len, batch_size, trg_vocab_size).to(self.device)\n",
        "        \n",
        "        #last hidden state of the encoder is the context\n",
        "        src = src[0,:]\n",
        "        hid_list = []\n",
        "        for j in range(512):\n",
        "          hid_list.append(1)\n",
        "        hid_data = torch.tensor([hid_list], device = self.device, dtype = torch.float32)\n",
        "        for i in range(input_length):\n",
        "           encoder_hidden = self.encoder(torch.tensor([src[i]], device = self.device), hid_data)\n",
        "           hid_data = encoder_hidden\n",
        "        context = encoder_hidden\n",
        "        \n",
        "        #context also used as the initial hidden state of the decoder\n",
        "        hidden = context\n",
        "        \n",
        "        #first input to the decoder is the <sos> tokens\n",
        "        trg = trg[0,:]\n",
        "        input = trg[0]\n",
        "        \n",
        "        for t in range(1, trg_len):\n",
        "            \n",
        "            #insert input token embedding, previous hidden state and the context state\n",
        "            #receive output tensor (predictions) and new hidden state\n",
        "            output, hidden = self.decoder(input, hidden, context)\n",
        "            \n",
        "            #place predictions in a tensor holding predictions for each token\n",
        "            outputs[t] = output\n",
        "            \n",
        "            #decide if we are going to use teacher forcing or not\n",
        "            teacher_force = random.random() < teacher_forcing_ratio\n",
        "            \n",
        "            #get the highest predicted token from our predictions\n",
        "            top1 = output.argmax(1) \n",
        "            \n",
        "            #if teacher forcing, use actual next token as next input\n",
        "            #if not, use predicted token\n",
        "            input = trg[t] if teacher_force else top1[0]\n",
        "\n",
        "        return outputs"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "define Seq2Seq Class"
      ],
      "metadata": {
        "id": "MKiHaRarUQzE"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 177,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "uITsSlNFJ0xK",
        "outputId": "992529a3-5c15-483c-d548-7f75be369cf4"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "Seq2Seq(\n",
              "  (encoder): Encoder(\n",
              "    (embedding): Embedding(10000, 256)\n",
              "    (rnn): GRU(256, 512)\n",
              "    (dropout): Dropout(p=0.5, inplace=False)\n",
              "  )\n",
              "  (decoder): Decoder(\n",
              "    (embedding): Embedding(10000, 256)\n",
              "    (rnn): GRU(768, 512)\n",
              "    (fc_out): Linear(in_features=1280, out_features=10000, bias=True)\n",
              "    (dropout): Dropout(p=0.5, inplace=False)\n",
              "  )\n",
              ")"
            ]
          },
          "metadata": {},
          "execution_count": 177
        }
      ],
      "source": [
        "INPUT_DIM = len(voc)\n",
        "OUTPUT_DIM = len(voc)\n",
        "ENC_EMB_DIM = 256\n",
        "DEC_EMB_DIM = 256\n",
        "HID_DIM = 512\n",
        "ENC_DROPOUT = 0.5\n",
        "DEC_DROPOUT = 0.5\n",
        "\n",
        "enc = Encoder(INPUT_DIM, ENC_EMB_DIM, HID_DIM, ENC_DROPOUT)\n",
        "dec = Decoder(OUTPUT_DIM, DEC_EMB_DIM, HID_DIM, DEC_DROPOUT)\n",
        "\n",
        "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
        "\n",
        "model = Seq2Seq(enc, dec, device).to(device)\n",
        "model.to(device)\n",
        "\n",
        "def init_weights(m):\n",
        "    for name, param in m.named_parameters():\n",
        "        nn.init.normal_(param.data, mean=0, std=0.01)\n",
        "        \n",
        "model.apply(init_weights)"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "تعیین پارامتر های مدل"
      ],
      "metadata": {
        "id": "Ppkg2uihUliT"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 178,
      "metadata": {
        "id": "zmOdq_qgKYjB"
      },
      "outputs": [],
      "source": [
        "optimizer = optim.Adam(model.parameters())\n",
        "PAD_IDX = voc.stoi['<PAD>']\n",
        "\n",
        "criterion = nn.CrossEntropyLoss(ignore_index = PAD_IDX)"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "آماده سازی برای آموزش"
      ],
      "metadata": {
        "id": "cjdI2xpGU-eb"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 179,
      "metadata": {
        "id": "_VB7nIRMMbYZ"
      },
      "outputs": [],
      "source": [
        "def train(model, iterator, optimizer, criterion, clip):\n",
        "    \n",
        "    model.train()\n",
        "    \n",
        "    epoch_loss = 0\n",
        "    \n",
        "    for first, rest in iterator:\n",
        "        \n",
        "        src = first\n",
        "        src = src.to(\"cuda\")\n",
        "        trg = rest\n",
        "        trg = trg.to(\"cuda\")\n",
        "        trg_len = trg.shape[1]\n",
        "        \n",
        "        optimizer.zero_grad()\n",
        "        \n",
        "        output = model(src, trg)\n",
        "        output.to(device)\n",
        "        \n",
        "        \n",
        "        output_dim = output.shape[-1]\n",
        "        \n",
        "        output = output[1:].view(-1, output_dim)\n",
        "        trg = trg[0,:]\n",
        "        trg = trg[0:trg_len - 1]\n",
        "        \n",
        "        \n",
        "        loss = criterion(output, trg)\n",
        "        \n",
        "        loss.backward()\n",
        "        \n",
        "        torch.nn.utils.clip_grad_norm_(model.parameters(), clip)\n",
        "        \n",
        "        optimizer.step()\n",
        "        \n",
        "        epoch_loss += loss.item()\n",
        "        \n",
        "    return epoch_loss / len(iterator)"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def evaluate(model, iterator, criterion):\n",
        "    \n",
        "    model.eval()\n",
        "    \n",
        "    epoch_loss = 0\n",
        "    \n",
        "    with torch.no_grad():\n",
        "    \n",
        "        for first, rest in iterator:\n",
        "\n",
        "            src = first\n",
        "            src = src.to(\"cuda\")\n",
        "            trg = rest\n",
        "            trg = trg.to(\"cuda\")\n",
        "            trg_len = trg.shape[1]\n",
        "\n",
        "            output = model(src, trg, 0) #turn off teacher forcing\n",
        "\n",
        "\n",
        "            output_dim = output.shape[-1]\n",
        "            \n",
        "            output = output[1:].view(-1, output_dim)\n",
        "            trg = trg[0,:]\n",
        "            trg = trg[0:trg_len - 1]\n",
        "\n",
        "\n",
        "            loss = criterion(output, trg)\n",
        "\n",
        "            epoch_loss += loss.item()\n",
        "        \n",
        "    return epoch_loss / len(iterator)"
      ],
      "metadata": {
        "id": "Jb3X2u4PVSiw"
      },
      "execution_count": 180,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "تعریف توابع آموزش و تست"
      ],
      "metadata": {
        "id": "rDZp8iQFWFBZ"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 181,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "1DOzTymJMyUO",
        "outputId": "3cc64c1a-4ec4-4b29-b99f-79321e4cc0ae"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch: 01\n",
            "\tTrain Loss: 5.475 | Train PPL: 238.576\n",
            "\t Test. Loss: 5.595 |  Test. PPL: 269.146\n",
            "Epoch: 02\n",
            "\tTrain Loss: 4.153 | Train PPL:  63.610\n",
            "\t Test. Loss: 5.743 |  Test. PPL: 312.029\n",
            "Epoch: 03\n",
            "\tTrain Loss: 3.648 | Train PPL:  38.380\n",
            "\t Test. Loss: 5.682 |  Test. PPL: 293.391\n",
            "Epoch: 04\n",
            "\tTrain Loss: 3.301 | Train PPL:  27.152\n",
            "\t Test. Loss: 5.878 |  Test. PPL: 357.187\n",
            "Epoch: 05\n",
            "\tTrain Loss: 3.062 | Train PPL:  21.368\n",
            "\t Test. Loss: 6.154 |  Test. PPL: 470.804\n",
            "Epoch: 06\n",
            "\tTrain Loss: 2.839 | Train PPL:  17.096\n",
            "\t Test. Loss: 6.344 |  Test. PPL: 568.989\n",
            "Epoch: 07\n",
            "\tTrain Loss: 2.648 | Train PPL:  14.130\n",
            "\t Test. Loss: 6.317 |  Test. PPL: 553.726\n",
            "Epoch: 08\n",
            "\tTrain Loss: 2.531 | Train PPL:  12.563\n",
            "\t Test. Loss: 7.164 |  Test. PPL: 1291.727\n",
            "Epoch: 09\n",
            "\tTrain Loss: 2.410 | Train PPL:  11.130\n",
            "\t Test. Loss: 6.192 |  Test. PPL: 488.593\n",
            "Epoch: 10\n",
            "\tTrain Loss: 2.333 | Train PPL:  10.309\n",
            "\t Test. Loss: 6.693 |  Test. PPL: 806.410\n"
          ]
        }
      ],
      "source": [
        "N_EPOCHS = 10\n",
        "CLIP = 1\n",
        "\n",
        "\n",
        "for epoch in range(N_EPOCHS):\n",
        "    \n",
        "    \n",
        "    train_loss = train(model, train_loader, optimizer, criterion, CLIP)\n",
        "    test_loss = evaluate(model, test_loader, criterion)\n",
        "    \n",
        "    \n",
        "    \n",
        "    print(f'Epoch: {epoch+1:02}')\n",
        "    print(f'\\tTrain Loss: {train_loss:.3f} | Train PPL: {math.exp(train_loss):7.3f}')\n",
        "    print(f'\\t Test. Loss: {test_loss:.3f} |  Test. PPL: {math.exp(test_loss):7.3f}')"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "مدل را تنها بر روی 1000 جمله اموزش داده ایم و همانطور که میبینید مدل به سمت بیش تطابقی میرود"
      ],
      "metadata": {
        "id": "GA-teB9lcxIo"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "در ادامه مدل را بر روی ده داده تست میکنیم"
      ],
      "metadata": {
        "id": "x3NhTZd0aC-D"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "count = 0\n",
        "for data, label in train_loader:\n",
        "  if(count == 10):\n",
        "    break\n",
        "  count += 1\n",
        "  data = data.to(device)\n",
        "  label = label.to(device)\n",
        "  print(\"our first five words:\", data)\n",
        "  print(\"rest of the sentence:\", label)\n",
        "  print(\"\\n\")\n",
        "  model.eval()\n",
        "  output = model(data, label, 0)\n",
        "  output_dim = output.shape[-1]\n",
        "  output = output[1:].view(-1, output_dim)\n",
        "  rest_of_the_sentence = []\n",
        "  for i in range(output.shape[0]):\n",
        "    word = torch.argmax(output[i])\n",
        "    rest_of_the_sentence.append(word.tolist())\n",
        "  print(\"our prediction is:\", rest_of_the_sentence)\n",
        "  print(\"this is for round:\", count, \"\\n\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "6grTpBjiajsM",
        "outputId": "c6ce10e3-aedf-4d8b-e101-55a5e323be93"
      },
      "execution_count": 190,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "our first five words: tensor([[   1,   15,   17, 4751,    3,    3,    2]], device='cuda:0')\n",
            "rest of the sentence: tensor([[   1,    3,   17, 1610,    3, 5830,    3, 1422,    6,   71,    3,    5,\n",
            "            3,   18, 4127, 9830,    3, 1794,   31, 1252,   16, 2856,  456,    5,\n",
            "          521,  453,    9,   10,   16,  521,  318, 6380,  273,   11,   29,    5,\n",
            "           67,  806,   10,    6, 1748,    5, 1252, 1112,   77,   45,   11,  172,\n",
            "         2557,   19,   13, 6945,    5, 3385,   41,    2]], device='cuda:0')\n",
            "\n",
            "\n",
            "our prediction is: [1, 593, 2719, 8280, 11, 11, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3]\n",
            "this is for round: 1 \n",
            "\n",
            "our first five words: tensor([[   1,   38, 1610, 5830,  700,   30,    2]], device='cuda:0')\n",
            "rest of the sentence: tensor([[   1, 1770,    9,   10,    8,  700,   14, 1610,    3,    3,    5,    3,\n",
            "            3,    3,   35,    3,  126,   20,    9,    2]], device='cuda:0')\n",
            "\n",
            "\n",
            "our prediction is: [1, 3, 3, 4987, 4987, 9, 9, 10, 8, 8, 8, 8, 8, 8, 8, 8, 3, 3, 3]\n",
            "this is for round: 2 \n",
            "\n",
            "our first five words: tensor([[   1,  470, 1610, 5830, 5476,    5,    2]], device='cuda:0')\n",
            "rest of the sentence: tensor([[   1,  630,  215,   18, 4127,   16, 2856,  456,    6,  789,   88,   14,\n",
            "            3,  388,    9,    2]], device='cuda:0')\n",
            "\n",
            "\n",
            "our prediction is: [1, 441, 1999, 8280, 5735, 9, 9, 10, 4, 1003, 1003, 1003, 3778, 3778, 3778]\n",
            "this is for round: 3 \n",
            "\n",
            "our first five words: tensor([[   1, 1610, 2544,  483,    4,   81,    2]], device='cuda:0')\n",
            "rest of the sentence: tensor([[   1,  560,  655, 1704,    3,  283,    3,    6,   71,    3,   24,    3,\n",
            "         1964,    3,  273,   23,    2]], device='cuda:0')\n",
            "\n",
            "\n",
            "our prediction is: [1, 560, 655, 1627, 1627, 1300, 1964, 1964, 1964, 1964, 1964, 1964, 1964, 1964, 1964, 1964]\n",
            "this is for round: 4 \n",
            "\n",
            "our first five words: tensor([[   1, 1209, 3393,   19,    3,    3,    2]], device='cuda:0')\n",
            "rest of the sentence: tensor([[ 1,  5,  3,  3, 91,  2]], device='cuda:0')\n",
            "\n",
            "\n",
            "our prediction is: [1, 593, 2719, 8280, 5313]\n",
            "this is for round: 5 \n",
            "\n",
            "our first five words: tensor([[   1,   51,  480, 1500, 8065, 1610,    2]], device='cuda:0')\n",
            "rest of the sentence: tensor([[   1,    3, 4648, 1610, 5830,   13, 1152,   11,   41,    2]],\n",
            "       device='cuda:0')\n",
            "\n",
            "\n",
            "our prediction is: [1, 3, 3, 3, 3, 229, 229, 816, 816]\n",
            "this is for round: 6 \n",
            "\n",
            "our first five words: tensor([[   1, 1664,   14, 2755,  142,   12,    2]], device='cuda:0')\n",
            "rest of the sentence: tensor([[   1, 3170,    4,   50,    3, 3404,   91,    2]], device='cuda:0')\n",
            "\n",
            "\n",
            "our prediction is: [1, 69, 69, 69, 69, 9, 9]\n",
            "this is for round: 7 \n",
            "\n",
            "our first five words: tensor([[   1,   65, 1664,   14, 3334,  295,    2]], device='cuda:0')\n",
            "rest of the sentence: tensor([[   1,   51,    4,  635, 8400,    5, 8657,    6,   12, 3170, 2670,   11,\n",
            "         8279,    2]], device='cuda:0')\n",
            "\n",
            "\n",
            "our prediction is: [1, 3778, 3778, 8280, 8280, 5, 5, 4, 4, 1879, 1879, 34, 34]\n",
            "this is for round: 8 \n",
            "\n",
            "our first five words: tensor([[  1,   3,   3,   8, 229, 641,   2]], device='cuda:0')\n",
            "rest of the sentence: tensor([[   1, 1193,   33,  583, 6561,   47,    8,  430, 3170,  927,  190,    8,\n",
            "          976,  420,    3,   44,    9,   10,  214,    8, 3343,    8,    3,    6,\n",
            "         1610, 2544,  483,  202,   11,   29,    2]], device='cuda:0')\n",
            "\n",
            "\n",
            "our prediction is: [1, 630, 630, 630, 630, 630, 630, 630, 630, 630, 630, 630, 630, 630, 630, 630, 630, 630, 630, 630, 630, 630, 630, 630, 630, 630, 630, 630, 630, 630]\n",
            "this is for round: 9 \n",
            "\n",
            "our first five words: tensor([[  1,   4, 146, 260,  88,  14,   2]], device='cuda:0')\n",
            "rest of the sentence: tensor([[   1, 1610, 5830,   33,  671, 1317,    3,    3,  214,    8,    3,  371,\n",
            "          799,   72,   27,   10,  214,    8,  241,    5,  800,  371, 1037,    8,\n",
            "           19,   22, 1045,    9,    2]], device='cuda:0')\n",
            "\n",
            "\n",
            "our prediction is: [1, 5533, 1999, 3, 3, 228, 228, 228, 228, 228, 228, 228, 228, 228, 228, 228, 228, 228, 228, 228, 228, 228, 228, 228, 228, 228, 228, 228]\n",
            "this is for round: 10 \n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "به دلیل بیش انطباقی بر روی داده های تست آزمایش انجام دادیم. و میتوان توسط دیکشنری این آرایه ها را به حروف تبدیل کرد"
      ],
      "metadata": {
        "id": "4PGXzrdfnoZ3"
      }
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "provenance": []
    },
    "gpuClass": "standard",
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}